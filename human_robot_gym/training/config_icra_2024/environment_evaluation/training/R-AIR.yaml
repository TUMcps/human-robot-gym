# Reach training with soft actor-critic, reference state initialization, and action-based imitation reward.
robot:
  name: Schunk
  controller_config_path: controllers/failsafe_controller/config/failsafe.json
  robot_config_path: models/robots/config/schunk.json
environment:
  env_id: ReachHuman
  robot_base_offset:
  - 0.0
  - 0.0
  - 0.0
  env_configuration: default
  controller_configs: null
  gripper_types: default
  initialization_noise: default
  use_camera_obs: false
  use_object_obs: true
  has_renderer: false
  has_offscreen_renderer: false
  render_camera: null
  render_collision_mesh: false
  render_visual_mesh: true
  render_gpu_device_id: -1
  control_freq: 10
  horizon: 100
  ignore_done: true
  hard_reset: false
  camera_names: frontview
  camera_heights: 256
  camera_widths: 256
  camera_depths: false
  camera_segmentations: null
  renderer: mujoco
  renderer_config: null
  shield_type: SSM
  visualize_failsafe_controller: false
  visualize_pinocchio: false
  control_sample_time: 0.004
  human_animation_names:
  - CMU/62_01
  - CMU/62_03
  - CMU/62_04
  - CMU/62_07
  - CMU/62_09
  - CMU/62_10
  - CMU/62_12
  - CMU/62_13
  - CMU/62_14
  - CMU/62_15
  - CMU/62_16
  - CMU/62_18
  - CMU/62_19
  base_human_pos_offset:
  - 0.0
  - 0.0
  - 0.0
  human_animation_freq: 120
  human_rand:
  - 0.0
  - 0.0
  - 0.0
  n_animations_sampled_per_100_steps: 5
  safe_vel: 0.01
  self_collision_safety: 0.012
  seed: ${run.seed}
  verbose: false
  table_full_size:
  - 1.5
  - 2.0
  - 0.05
  table_friction:
  - 1.0
  - 0.005
  - 0.0001
  reward_scale: 1.0
  reward_shaping: true
  goal_dist: 0.1
  n_goals_sampled_per_100_steps: 20
  collision_reward: 0
  task_reward: 1
  object_placement_initializer: null
  obstacle_placement_initializer: null
  done_at_collision: false
  done_at_success: true
  randomize_initial_pos: false
wrappers:
  collision_prevention:
    replace_type: 0
    n_resamples: 20
  action_based_expert_imitation_reward:
    dataset_name: ${run.dataset_name}
    alpha: 0.25
    rsi_prob: 0.5
    beta: 1.0
    iota_m: 0.1
    iota_g: 0.5
    m_sim_fn: gaussian
    g_sim_fn: gaussian
    normalize_joint_actions: false
  dataset_obs_norm:
    dataset_name: ${run.dataset_name}
    squash_factor: null
    allow_different_observation_shapes: false
    mean: null
    std: null
algorithm:
  name: SAC
  policy: MlpPolicy
  learning_rate: 0.0005
  buffer_size: 1000000
  learning_starts: 1000
  batch_size: 128
  tau: 0.005
  gamma: 0.99
  train_freq:
  - 100
  - step
  gradient_steps: -1
  replay_buffer_kwargs: null
  optimize_memory_usage: false
  ent_coef: auto_0.2
  target_update_interval: 1
  target_entropy: auto
  use_sde: false
  sde_sample_freq: -1
  use_sde_at_warmup: false
  create_eval_env: false
  policy_kwargs:
    net_arch:
    - 64
    - 64
    - 64
  verbose: 0
  seed: ${run.seed}
  device: auto
  _init_setup_model: true
  action_noise: null
expert:
  id: ReachHuman
  signal_to_noise_ratio: 0.98
  delta_time: 0.01
  seed: ${run.seed}
  obs_keys:
  - goal_difference
run:
  n_envs: 8
  n_steps: 1000001
  save_freq: 50000
  test_only: false
  load_step: null
  id: null
  type: tensorboard
  log_interval:
  - 800
  - step
  seed: null
  eval_seed: null
  start_index: 0
  n_test_episodes: 20
  env_type: env
  obs_keys:
  - goal_difference
  expert_obs_keys: ${expert.obs_keys}
  log_info_keys:
  - n_goal_reached
  - collision
  - collision_type
  - n_collisions
  - n_collisions_static
  - n_collisions_robot
  - n_collisions_human
  - n_collisions_critical
  - timeout
  - failsafe_interventions
  - action_resamples
  - im_rew_mean
  - env_rew_mean
  - full_rew_mean
  - ep_im_rew_mean
  - ep_env_rew_mean
  - ep_full_rew_mean
  - g_im_rew_mean
  - m_im_rew_mean
  - ep_g_im_rew_mean
  - ep_m_im_rew_mean
  monitor_dir: null
  vec_env_kwargs: null
  monitor_kwargs: null
  verbose: true
  resetting_interval: null
  dataset_name: human-reach
wandb_run:
  project: icra_2024
  entity: null
  group: R_AIR
  name: run_${run.seed}
  tags: null
